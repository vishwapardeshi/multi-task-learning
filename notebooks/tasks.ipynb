{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src/ to Python path\n",
    "sys.path.append(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "For Task 1, I set up a sentence encoding pipeline using the bert-base-uncased model from Hugging Face. The goal was to encode input sentences into fixed-length embeddings that could be used for downstream tasks such as classification or similarity.\n",
    "\n",
    "Initially, I used the raw BERT model and implemented mean pooling over the last hidden states (excluding padding tokens). This approach is often more effective than using the [CLS] token for general-purpose sentence embeddings.\n",
    "\n",
    "As the project evolved, I refactored this logic into a reusable utility function called encode_sentences(), which supports both mean and CLS pooling. To align with the multi-task design, I also reused the MultiTaskModel backbone to extract embeddings. This made the architecture consistent and reusable across all tasks.\n",
    "\n",
    "To evaluate the quality of the embeddings, I computed cosine similarity between related and unrelated sentence pairs. Related sentences (e.g., about NLP or Paris) had higher similarity scores, demonstrating that the embeddings effectively captured semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data import encode_sentences\n",
    "from model import MultiTaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 768])\n",
      "üîç Cosine Similarities Between Sentence Pairs:\n",
      "(0, 1) 'I love natural language processing.' <-> 'NLP is a fascinating field.' -> similarity = 0.6150\n",
      "(0, 2) 'I love natural language processing.' <-> 'The Eiffel Tower is in Paris.' -> similarity = 0.5189\n",
      "(2, 4) 'The Eiffel Tower is in Paris.' <-> 'Paris is the capital of France.' -> similarity = 0.7452\n",
      "(1, 3) 'NLP is a fascinating field.' <-> 'Transformers are used in deep learning.' -> similarity = 0.6954\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"NLP is a fascinating field.\",\n",
    "    \"The Eiffel Tower is in Paris.\",\n",
    "    \"Transformers are used in deep learning.\",\n",
    "    \"Paris is the capital of France.\"\n",
    "]\n",
    "\n",
    "# Similar sentence pairs: [0,1] and [2,4] should have high similarity\n",
    "pairs = [(0, 1), (0, 2), (2, 4), (1, 3)]\n",
    "\n",
    "model = MultiTaskModel(\"bert-base-uncased\", 11, 3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "embeddings = encode_sentences(sentences, model, tokenizer, pooling=\"mean\")\n",
    "print(embeddings.shape)\n",
    "\n",
    "print(\"üîç Cosine Similarities Between Sentence Pairs:\")\n",
    "for i, j in pairs:\n",
    "    sim = F.cosine_similarity(embeddings[i], embeddings[j], dim=0).item()\n",
    "    print(f\"({i}, {j}) '{sentences[i]}' <-> '{sentences[j]}' -> similarity = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "For Task 2, I expanded the sentence transformer setup to support multi-task learning. Since I chose sentence classification and sentiment analysis as my two tasks, I leveraged the TweetEval dataset, which includes both emotion and sentiment labels. This made it a practical, real-world source for aligned multitask inputs.\n",
    "\n",
    "To support this setup, I implemented a shared BERT backbone with two task-specific classification heads:\n",
    "\n",
    "classifier_a for emotion classification\n",
    "\n",
    "classifier_b for sentiment classification\n",
    "\n",
    "This design allows both tasks to benefit from the same sentence representation while learning their own task-specific nuances. The loss function is computed separately for each task and summed (loss = loss_a + loss_b) during training. This setup lays the foundation for scalable multi-task learning by simply plugging in new task heads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "For Task 3, I explored how freezing different parts of the model affects training and learning dynamics.\n",
    "\n",
    "Freezing the entire model turns it into a fixed feature extractor. This approach is fast and useful for quick baselines or extremely small datasets, but it limits the model‚Äôs ability to adapt.\n",
    "\n",
    "Freezing the transformer backbone allows only the classification heads to learn. This is effective when using a strong pretrained model and prevents overfitting to limited task-specific data.\n",
    "\n",
    "Freezing one of the task-specific heads is useful when you want to retain the performance of a stable task while improving another. For example, if the sentiment task performs well and doesn‚Äôt require further tuning, freezing its head allows the emotion head to learn without interference.\n",
    "\n",
    "In terms of transfer learning, I started with the bert-base-uncased model as the backbone. This model captures general-purpose linguistic patterns. I chose to freeze the lower layers (which handle syntax) and fine-tune the upper layers and task heads (which are more semantic and task-specific). This approach offers a good balance between stability and adaptability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "For Task 4, I created a reusable MultiTaskTrainer class that encapsulates training logic in a clean, modular way. Inspired by PyTorch Lightning, I split the setup into:\n",
    "\n",
    "prepare_data() for downloading and preparing data\n",
    "\n",
    "setup() for building datasets and dataloaders\n",
    "\n",
    "train() for the training loop and loss tracking\n",
    "\n",
    "This structure makes the training logic clean, extensible, and testable.\n",
    "\n",
    "To validate correctness, I implemented an overfit mode that trains the model on a single batch. By increasing the learning rate and training for more epochs, the model learns to perfectly memorize the batch ‚Äî a key sanity check for multi-task setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from train import MultiTaskTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45615/45615 [00:00<00:00, 309556.38 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12284/12284 [00:00<00:00, 655040.05 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 252114.57 examples/s]\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3257/3257 [00:00<00:00, 536497.98 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1421/1421 [00:00<00:00, 154848.17 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 374/374 [00:00<00:00, 83444.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Avg Loss: 3.5471\n",
      "Epoch 2/30, Avg Loss: 2.2816\n",
      "Epoch 3/30, Avg Loss: 1.2555\n",
      "Epoch 4/30, Avg Loss: 0.8170\n",
      "Epoch 5/30, Avg Loss: 0.5313\n",
      "Epoch 6/30, Avg Loss: 0.3137\n",
      "Epoch 7/30, Avg Loss: 0.2171\n",
      "Epoch 8/30, Avg Loss: 0.1606\n",
      "Epoch 9/30, Avg Loss: 0.1331\n",
      "Epoch 10/30, Avg Loss: 0.0935\n",
      "Epoch 11/30, Avg Loss: 0.0762\n",
      "Epoch 12/30, Avg Loss: 0.0614\n",
      "Epoch 13/30, Avg Loss: 0.0480\n",
      "Epoch 14/30, Avg Loss: 0.0435\n",
      "Epoch 15/30, Avg Loss: 0.0360\n",
      "Epoch 16/30, Avg Loss: 0.0333\n",
      "Epoch 17/30, Avg Loss: 0.0290\n",
      "Epoch 18/30, Avg Loss: 0.0271\n",
      "Epoch 19/30, Avg Loss: 0.0259\n",
      "Epoch 20/30, Avg Loss: 0.0247\n",
      "Epoch 21/30, Avg Loss: 0.0229\n",
      "Epoch 22/30, Avg Loss: 0.0213\n",
      "Epoch 23/30, Avg Loss: 0.0178\n",
      "Epoch 24/30, Avg Loss: 0.0190\n",
      "Epoch 25/30, Avg Loss: 0.0173\n",
      "Epoch 26/30, Avg Loss: 0.0169\n",
      "Epoch 27/30, Avg Loss: 0.0153\n",
      "Epoch 28/30, Avg Loss: 0.0138\n",
      "Epoch 29/30, Avg Loss: 0.0141\n",
      "Epoch 30/30, Avg Loss: 0.0133\n"
     ]
    }
   ],
   "source": [
    "trainer = MultiTaskTrainer(\n",
    "    batch_size=8,\n",
    "    epochs=30,            \n",
    "    sample_size=8,        \n",
    "    lr=1e-4 \n",
    ")\n",
    "trainer.prepare_data()\n",
    "trainer.setup()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Classification Predictions: [3, 0, 0, 1, 1, 0, 0, 0]\n",
      "Labels: [3, 0, 0, 1, 1, 0, 0, 0]\n",
      "Sentiment Analysis Predictions: [1, 2, 1, 2, 2, 0, 0, 2]\n",
      "Labels: [1, 2, 1, 2, 2, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in trainer.dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(trainer.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(trainer.device)\n",
    "        labels_a = batch[\"label_a\"].to(trainer.device)\n",
    "        labels_b = batch[\"label_b\"].to(trainer.device)\n",
    "\n",
    "        out_a, out_b = trainer.model(input_ids, attention_mask)\n",
    "        pred_a = out_a.argmax(dim=1)\n",
    "        pred_b = out_b.argmax(dim=1)\n",
    "\n",
    "        print(\"Sentence Classification Predictions:\", pred_a.cpu().tolist())\n",
    "        print(\"Labels:\", labels_a.cpu().tolist())\n",
    "\n",
    "        print(\"Sentiment Analysis Predictions:\", pred_b.cpu().tolist())\n",
    "        print(\"Labels:\", labels_b.cpu().tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "second",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
